## This file was generated by Pentaho Data Integration version 6.1.0.1-196.
#
# Here are a few examples of variables to set:
#
# DEVELOPMENT_SERVER = hccdevne
#
# Note: lines like these with a # in front of it are comments
#
#
#Wed Jan 11 15:49:00 BST 2017

PENTAHO_HOME= "{{ PENTAHO_HOME}}"
STAGINGAREA= {{STAGINGAREA}}

# Connection details for the SQL DB used for metadata and logs.
SQLDBPORT= {{ SQLDBPORT }}
SQLDBUSER= {{SQLDBUSER}}
SQLDBNAME= {{SQLDBNAME}}
SQLDBPASSWORD= {{SQLDBPASSWORD}}
SQLHOSTNAME= {{ SQLHOSTNAME }}

# Use format "hdfs://HOST_NAME"
HDFSHOST= {{HDFSHOST}}
# Use format ":PORT_NUM"
HDFSPORT= {{HDFSPORT}}
# Use format "/USER/USER_NAME"
HDFSUSER= {{HDFSUSER}}

# This should just be the job tracker, but is also used for the hdfs host.
HADOOPHOST= {{HADOOPHOST}}

# IT1 - depot servers to queue producer
# The number of threads we use to get files from FTP, decompress, copy to hadoop and clean up.
ARCHIVEBATCHSIZE= {{ARCHIVEBATCHSIZE}}


# IT2 - queue consumer to impala

# This controls the number of files we put into each LIST file for the map-reduce job.
DATABATCHSIZE= {{DATABATCHSIZE}}
# The degree of parallelism we use when running the parser map reduce process.
# Note: This can cause high impala load when parsing completes.
DATAINGESTIONCOPIES= {{DATAINGESTIONCOPIES}}


# Impala connection details
IMPALAHOST= {{IMPALAHOST}}
#IMPALAPASSWORD=Encrypted 2be98afc86aa7f2ac883aaf74d39ba18b
IMPALAPASSWORD= {{IMPALAPASSWORD}}
IMPALADBNAME= {{IMPALADBNAME}}
#IMPALAUSER=hccadmin1
IMPALAUSER= {{IMPALAUSER}}
IMPALAPORT= {{IMPALAPORT}}

# This is the staging schema, and also is appended by _tmp to identify the working schema
IMPALASCHEMA= {{IMPALASCHEMA}}

# This is used for the dimensions
IMPALADWHSCHEMA= {{IMPALADWHSCHEMA}}
# This is used for the PCS
FILE_NAME_FACT_PCS= {{FILE_NAME_FACT_PCS}}
STAGING_PCS_SD1= {{STAGING_PCS_SD1}}
STAGING_PCS_SD7= {{STAGING_PCS_SD7}}
STAGING_PRE_PCS_SD1= {{STAGING_PRE_PCS_SD1}}

#This is used for Event Fact
FILE_NAME_FACT_EVENT= {{FILE_NAME_FACT_EVENT}}
FILE_NAME_OLD_FACT_EVENT= {{FILE_NAME_OLD_FACT_EVENT}}


# This is used for energyX
STAGING_FACT_ENERGYX= {{STAGING_FACT_ENERGYX}}
FACT_ENERGYX= {{FACT_ENERGYX}}

# The default location of the hive warehouse folder. Doesnt usually change.
IMPALASCHEMAPATH= {{IMPALASCHEMAPATH}}


# Path (used in HDFS?) and the schema/table name used for the timeseries data
TIMESERIESPATH= {{TIMESERIESPATH}}
TIMESERIESIMPALASCHEMA= {{TIMESERIESIMPALASCHEMA}}
TIMESERIESTABLENAME= {{TIMESERIESTABLENAME}}


# Activemq
QUEUEHOST= {{QUEUEHOST}}

#Activemq queue name for the ANALYTICS processes
ANALYTICSQUEUE= {{ANALYTICSQUEUE}}


# Configuration used by the digital signal fact loader

# The number of binary files we process.  This is just a LIMIT on a SQL query that has no particular order.
# We'll have to find a good value - should be more than 1, but sufficient so the job doesnt bog down and take ages.
DIGSIGFACTFILESTOLOAD= {{DIGSIGFACTFILESTOLOAD}}
CNTSIGFACTFILESTOLOAD= {{CNTSIGFACTFILESTOLOAD}}

# The degree of parallelism we use when running the parser map reduce process for digital signal.
# Note: This can cause high impala load when parsing completes
DIGSIGFACTCOPIES= {{DIGSIGFACTCOPIES}}
CNTSIGFACTCOPIES= {{CNTSIGFACTCOPIES}}

# Degree of parallelism we use when getting data from different types of SD impala tables and provide it to timeseries tables
TIMESERIESCOPIES= {{TIMESERIESCOPIES}}
# Max batch execution of data files at once
TIMESERIESMAXBATCH= {{TIMESERIESMAXBATCH}}

# Number of days to purge
PURGENUMBEROFDAYS= {{PURGENUMBEROFDAYS}}

# Number of days to keep of log data
PURGEREPODAYS= {{PURGEREPODAYS}}


#SCHEDULER - Host, port, user and user password of the BA server where the schedules will be created.
SCHEDULER_HOST= {{SCHEDULER_HOST}}
SCHEDULER_PORT= {{SCHEDULER_PORT}}
SCHEDULER_USER= {{SCHEDULER_USER}}
SCHEDULER_USER_PASS= {{SCHEDULER_USER_PASS}}

#Physical path where di folder from source is located.
REPOROOT= {{REPOROOT}}
#### Logs
KETTLE_TRANS_LOG_DB= {{KETTLE_TRANS_LOG_DB}}
KETTLE_TRANS_LOG_SCHEMA= {{KETTLE_TRANS_LOG_SCHEMA}}
KETTLE_TRANS_LOG_TABLE= {{KETTLE_TRANS_LOG_TABLE}}

KETTLE_JOB_LOG_DB= {{KETTLE_JOB_LOG_DB}}
KETTLE_JOB_LOG_SCHEMA= {{KETTLE_JOB_LOG_SCHEMA}}
KETTLE_JOB_LOG_TABLE= {{KETTLE_JOB_LOG_TABLE}}

KETTLE_CHANNEL_LOG_DB= {{KETTLE_CHANNEL_LOG_DB}}
KETTLE_CHANNEL_LOG_SCHEMA= {{KETTLE_CHANNEL_LOG_SCHEMA}}
KETTLE_CHANNEL_LOG_TABLE= {{KETTLE_CHANNEL_LOG_TABLE}}

#Essential additional detail in the logs - gives a 'Stack'
KETTLE_LOG_MARK_MAPPINGS= {{KETTLE_LOG_MARK_MAPPINGS}}


#TGZ file backup to HCP
HCPROOT= {{HCPROOT}}
HCPSERVER= {{HCPSERVER}}
HCPPORT= {{HCPPORT}}
HCPUSER= {{HCPUSER}}
HCPPASSWORD= {{HCPPASSWORD}}

#Operational DB Connection
OPERATIONALDBNAME= {{OPERATIONALDBNAME}}
OPERATIONALDBPORT= {{OPERATIONALDBPORT}}
OPERATIONALDBUSER= {{OPERATIONALDBUSER}}
OPERATIONALDBPASSWORD= {{OPERATIONALDBPASSWORD}}
OPERATIONALDBHOSTNAME= {{OPERATIONALDBHOSTNAME}}

#Cassandra DB Connection
CASSANDRAURL= {{CASSANDRAURL}}
CASSANDRAUSER= {{CASSANDRAUSER}}
CASSANDRAPASSWORD= {{CASSANDRAPASSWORD}}



#### DATA SCIENCE ENABLEMENT
# Staging folder for Dada science module
DSEXTRACTAREA= {{DSEXTRACTAREA}}
# Data Science number of jobs in parallel
DATASCIENCECOPIES= {{DATASCIENCECOPIES}}
